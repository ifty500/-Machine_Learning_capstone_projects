{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Dirac's Statistics Midterm\n",
    "\n",
    "Grading a multiple choice exam is easy. But how much do multiple choice exams tell us about what a student really knows? Dr. Dirac is administering a statistics midterm exam and wants to use Bayes’ Theorem to help him understand the following:\n",
    "\n",
    "Given that a student answered a question correctly, what is the probability that she really knows the material?\n",
    "Dr. Dirac knows the following probabilities based on many years of teaching:\n",
    "\n",
    "There is a question on the exam that 60% of students know the correct answer to.\n",
    "Given that a student knows the correct answer, there is still a 15% chance that the student picked the wrong answer.\n",
    "Given that a student does not know the answer, there is still a 20% chance that the student picks the correct answer by guessing.\n",
    "Using these probabilities, we can answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question \n",
    ".\n",
    "In order to use Bayes Theorem, we need to phrase our question as P(A|B).\n",
    "\n",
    "What is A and B in this case?\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "What is the probability that the student knows the material?\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Given that the student knows the material, what is the probability that she answers correctly?\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "What is the probability of any student answering correctly?\n",
    "\n",
    "\n",
    "\n",
    "5.\n",
    "Using the three probabilities and Bayes’ Theorem, calculate P(knows material | answers correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8644067796610169\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# P(A|B)=P(knows the material| answers correctly)\n",
    "\n",
    "# P(A) = P(knows_the_material) = 0.60\n",
    "# P(B|A) = P(answers correctly | knows the material) = 100 - 15 = 0.85\n",
    "\n",
    "# P(answers correctly | knows the material) * P(knows_the_material) = 0.85 * 0.60\n",
    "# P(answers correctly |does not know the material) * P( Not knows_the_material) = 0.20 * .40 (100-60) \n",
    "\n",
    "# P(B) = P(answers_correctly) = P(answers correctly | knows the material) * P(knows_the_material) + P(answers correctly |does not know the material) * P( Not knows_the_material) = 0.85 * 0.60 + 0.20 * .40 = 0.59\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A = \"knows the material\"\n",
    "B = \"answers correctly\"\n",
    "\n",
    "P_knows_the_material = 0.60\n",
    "P_answers_correctly_given_knows_the_material = 0.85\n",
    "\n",
    "P_answers_correctly = 0.85 * 0.60 + 0.20 * .40 \n",
    "\n",
    "\n",
    "P_knows_the_material_given_answers_correctly =((P_answers_correctly_given_knows_the_material * P_knows_the_material)/P_answers_correctly)\n",
    "\n",
    "print(P_knows_the_material_given_answers_correctly)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

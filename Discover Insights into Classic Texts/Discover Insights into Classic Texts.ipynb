{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover Insights into Classic Texts\n",
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project you will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: Oscar Wilde’s The Picture of Dorian Gray or Homer’s The Iliad! Fear not if you haven’t heard or read the novels, one of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!\n",
    "\n",
    "By the end of this project, you will find out the main topics of discussion in the novel of your choosing and can begin to discern some of the author’s thoughts and beliefs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "  # create a PunktSentenceTokenizer\n",
    "  sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "  # sentence tokenize text\n",
    "  sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "  \n",
    "  # create a list to hold word tokenized sentences\n",
    "  word_tokenized = list()\n",
    "  \n",
    "  # for-loop through each tokenized sentence in sentence_tokenized\n",
    "  for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "  return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "\n",
    "## Import and Preprocess Text Data\n",
    "\n",
    "1.\n",
    "Given to you in the code editor are text files for the The Picture of Dorian Gray, named dorian_gray.txt, and The Iliad, named the_iliad.txt, sourced from Project Gutenberg. Import the text of your choosing, convert it to lowercase, and name it text using the following line of code.\n",
    "\n",
    "text = open(\"_______.txt\",encoding='utf-8').read().lower()\n",
    "Replace the blank with the name of the text file for the novel you choose to analyze!\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "With the text imported, now you need to split the text into individual sentences and then individual words. This allows you to perform a sentence-by-sentence parsing analysis!\n",
    "\n",
    "Provided to you in the code editor is a customized function word_sentence_tokenize() that will sentence tokenize a text and then word tokenize each sentence, returning a list of word tokenized sentences. Call the function with text as an argument and save the result to a variable named word_tokenized_text.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Save any word tokenized sentence in word_tokenized_text to a variable named single_word_tokenized_sentence. Print single_word_tokenized_sentence as a check to visualize what you have done so far!\n",
    "\n",
    "\n",
    "\n",
    "Part-of-speech Tag Text\n",
    "4.\n",
    "Next you can part-of-speech tag each sentence to allow for syntax parsing! Begin by creating a list named pos_tagged_text that will hold each part-of-speech tagged sentence from the novel.\n",
    "\n",
    "\n",
    "\n",
    "5.\n",
    "Loop through each word tokenized sentence in word_tokenized_text and part-of-speech tag each sentence using nltk‘s pos_tag() function. Append the result to pos_tagged_text.\n",
    "\n",
    "\n",
    "\n",
    "6.\n",
    "Save any part-of-speech tagged sentence in pos_tagged_text to a variable named single_pos_sentence. Print single_pos_sentence as a check to visualize what you have done so far!\n",
    "\n",
    "\n",
    "\n",
    "Chunk Sentences\n",
    "7.\n",
    "Now that you have part-of-speech tagged your text, you can move on to syntax parsing!\n",
    "\n",
    "Begin by defining a piece of chunk grammar np_chunk_grammar that will chunk a noun phrase. Remember, a noun phrase consists of an optional determiner DT, followed by any number of adjectives JJ, followed by a noun NN.\n",
    "\n",
    "\n",
    "\n",
    "8.\n",
    "Create a nltk RegexpParser object named np_chunk_parser using the noun phrase chunk grammar you defined as an argument.\n",
    "\n",
    "\n",
    "\n",
    "9.\n",
    "Define a piece of chunk grammar named vp_chunk_grammar that will chunk a verb phrase of the following form: noun phrase, followed by a verb VB, followed by an optional adverb RB.\n",
    "\n",
    "\n",
    "\n",
    "10.\n",
    "Create a nltk RegexpParser object named vp_chunk_parser using the verb phrase chunk grammar you defined as an argument.\n",
    "\n",
    "\n",
    "\n",
    "11.\n",
    "Create two empty lists np_chunked_text and vp_chunked_text that will hold the chunked sentences from your text.\n",
    "\n",
    "\n",
    "\n",
    "12.\n",
    "Loop through each part-of-speech tagged sentence in pos_tagged_text and noun phrase chunk each sentence using your RegexpParser‘s .parse() method. Append the result to np_chunked_text.\n",
    "\n",
    "\n",
    "Hint:\n",
    "You can loop through each part-of-speech tagged sentence as shown below:\n",
    "\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "Then within the for-loop, noun phrase chunk each sentence with np_chunk_parser‘s .parse() method and append to np_chunked_text:\n",
    "\n",
    "    np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "\n",
    "13.\n",
    "Within the same loop you defined in the previous task, verb phrase chunk each part-of-speech tagged sentence using your RegexpParser‘s .parse() method. Append the result to vp_chunked_text.\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "Verb phrase chunk each part-of-speech tagged sentence with vp_chunk_parser‘s .parse() method and append to vp_chunked_text:\n",
    "\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged_sentence))\n",
    "    \n",
    "Make sure this line of code is within the same loop you defined in the last task!\n",
    "\n",
    "\n",
    "Analyze Chunks\n",
    "14.\n",
    "Now that you have chunked your novel, you can analyze the chunk frequencies to gain insights!\n",
    "\n",
    "A function np_chunk_counter() that returns the 30 most common NP-chunks from a list of chunked sentences has been imported to the workspace for you. Call np_chunk_counter() with np_chunked_text as an argument and save the result to a variable named most_common_np_chunks. Print most_common_np_chunks. What sticks out to you about the most common noun phrase chunks? Are you surprised by anything? Open the hint to see our analysis.\n",
    "\n",
    "Want to see how np_chunk_counter() works? Use the file navigator to open chunk_counters.py and inspect np_chunk_counter().\n",
    "\n",
    "\n",
    "Hint\n",
    "You can call np_chunk_counter() to count the most common noun phrase chunks as follows:\n",
    "\n",
    "    most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "    \n",
    "Print most_common_np_chunks to view the 30 most common chunks in descending order, given in the form (NP-Chunk, Count).\n",
    "\n",
    "Analysis for The Picture of Dorian Gray\n",
    "\n",
    "Looking at most_common_np_chunks, you can identify characters of importance in the text such as henry, harry, dorian gray, and basil, based on their frequency. Additionally another noun phrase the picture appears to be very relevant.\n",
    "\n",
    "Analysis for The Iliad\n",
    "\n",
    "Looking at most_common_np_chunks, you can identify characters of importance in the text such as hector and jove based on their frequency. Additionally a location of importance, troy, is mentioned often. A theme of war can also implied by its high frequency count.\n",
    "\n",
    "15.\n",
    "A function vp_chunk_counter() that returns the 30 most common VP-chunks from a list of chunked sentences has been imported to the workspace for you. Call vp_chunk_counter() with vp_chunked_text as an argument and save the result to a variable named most_common_vp_chunks. Print most_common_vp_chunks. What sticks out to you about the most common verb phrase chunks? Are you surprised by anything? Open the hint to see our analysis.\n",
    "\n",
    "Want to see how vp_chunk_counter() works? Use the file navigator to open chunk_counters.py and inspect vp_chunk_counter().\n",
    "\n",
    "\n",
    "Hint\n",
    "You can call vp_chunk_counter() to count the most common verb phrase chunks as follows:\n",
    "\n",
    "    most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "    \n",
    "Print most_common_vp_chunks to view the 30 most common chunks in descending order, given in the form (VP-Chunk, Count).\n",
    "\n",
    "Analysis for The Picture of Dorian Gray\n",
    "\n",
    "Looking at most_common_vp_chunks, some interesting findings appear. The verb phrases i want, i know and i have occur frequently, indicating a theme of desire and need.\n",
    "\n",
    "Analysis for The Iliad\n",
    "\n",
    "Looking at most_common_vp_chunks, you can see that verb phrases of the form you defined in your chunk grammar do not appear as often in The Iliad as noun phrases. This can indicate a different style of writing taken by the author that does not follow traditional grammatical style (i.e. poetry). Even when chunks are not found, their absence can give you insight!\n",
    "\n",
    "\n",
    "16.\n",
    "Amazing! You have performed a syntax parsing analysis on a novel and gained insight into both the meaning of the text and how the author thinks, without reading a page!\n",
    "\n",
    "Now’s your chance to get creative. Is there a different pattern of parts-of-speech you want to identify and count in the novel you selected? Add a new piece of chunk grammar and repeat the process of chunking. What do you find?\n",
    "\n",
    "17.\n",
    "Not the biggest fan of The Picture of Dorian Gray or The Iliad? No worries! Included in the file navigator is a blank text file named my_text.txt. Open the file and copy any text of your choice (novel, script, article, etc.) into the file. Save the file and then return to script.py. Update the opened text file to my_text.txt and rerun script.py to perform a syntax parsing analysis on your text! What insights or deeper meanings did you discover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "chunk structures must contain tagged tokens or trees",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-73174fe819f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpos_tagged_sentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;31m# chunk each sentence and append to lists here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[0mnp_chunked_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_chunk_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tagged_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[0mvp_chunked_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvp_chunk_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tagged_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m                 \u001b[0mchunk_struct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[0;32m   1096\u001b[0m             \u001b[0mtrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[0mchunkstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChunkString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m         \u001b[1;31m# Apply the sequence of rules to the chunkstring.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, chunk_struct, debug_level)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pieces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'><'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdebug_level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pieces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'><'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdebug_level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36m_tag\u001b[1;34m(self, tok)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chunk structures must contain tagged '\u001b[0m \u001b[1;34m'tokens or trees'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_verify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: chunk structures must contain tagged tokens or trees"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "#from tokenize_words import word_sentence_tokenize\n",
    "#from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "\n",
    "# import text of choice here\n",
    "text = open(\"dorian_gray.txt\",encoding='utf-8').read().lower()\n",
    "#text = open(\"the_iliad.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "#print(word_tokenized_text)\n",
    "\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text  = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for word_tokenized_sentence in word_tokenized_text:\n",
    "  pos_tagged_text.append(word_tokenized_sentence)\n",
    "\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "  \n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "\n",
    "single_pos_sentence = pos_tagged_text[100]\n",
    "#print(single_pos_sentence)\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser  = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text =[]\n",
    "\n",
    "vp_chunked_text =[]\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "\n",
    "print(most_common_np_chunks)\n",
    "\n",
    "# # store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "\n",
    "print(most_common_vp_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
